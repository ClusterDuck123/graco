{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import squareform, pdist, cdist\n",
    "from itertools import islice, combinations, product\n",
    "from pyclustering.cluster.kmedoids import kmedoids\n",
    "from functools import partial\n",
    "from random import sample\n",
    "\n",
    "import os\n",
    "import time\n",
    "import graco\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "sns.set()\n",
    "pd.set_option(\"display.max_columns\", 50)\n",
    "\n",
    "DATA_DIRECTORY = \"/home/clusterduck123/Desktop/git/supplements/data\"\n",
    "CPP_DIRECTORY = \"/home/clusterduck123/Desktop/git/graco/graco/cpp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DATA_DIRECTORY = f\"{DATA_DIRECTORY}/raw_data\"\n",
    "PPI_DIRECTORY = f\"{DATA_DIRECTORY}/PPI\"\n",
    "ANNOTATIONS_DIRECTORY = f\"{DATA_DIRECTORY}/annotations\"\n",
    "MATRIX_DIRECTORY = f\"{DATA_DIRECTORY}/matrix\"\n",
    "CLUSTERS_DIRECTORY = f\"{DATA_DIRECTORY}/clusters\"\n",
    "\n",
    "if not os.path.exists(DATA_DIRECTORY):\n",
    "    os.makedirs(DATA_DIRECTORY)\n",
    "        \n",
    "if not os.path.exists(RAW_DATA_DIRECTORY):\n",
    "    os.makedirs(RAW_DATA_DIRECTORY)     \n",
    "    \n",
    "if not os.path.exists(PPI_DIRECTORY):\n",
    "    os.makedirs(PPI_DIRECTORY)\n",
    "    \n",
    "if not os.path.exists(ANNOTATIONS_DIRECTORY):\n",
    "    os.makedirs(ANNOTATIONS_DIRECTORY)\n",
    "    \n",
    "if not os.path.exists(MATRIX_DIRECTORY):\n",
    "    os.makedirs(MATRIX_DIRECTORY)\n",
    "    \n",
    "if not os.path.exists(CLUSTERS_DIRECTORY):\n",
    "    os.makedirs(CLUSTERS_DIRECTORY)\n",
    "    \n",
    "if not os.path.exists(f\"{CLUSTERS_DIRECTORY}/GDV\"):\n",
    "    os.makedirs(f\"{CLUSTERS_DIRECTORY}/GDV\")\n",
    "    \n",
    "if not os.path.exists(f\"{CLUSTERS_DIRECTORY}/GCV\"):\n",
    "    os.makedirs(f\"{CLUSTERS_DIRECTORY}/GCV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distance matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PPI_nx = nx.read_edgelist(f\"{PPI_DIRECTORY}/BioGRID_sc.txt\")\n",
    "GDV = graco.orbits(PPI_nx)\n",
    "GCV = graco.coefficients(GDV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GDV-based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GDV similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(f\"{CPP_DIRECTORY}/matrix.in\", GDV, \n",
    "           header=f\"{len(GDV)} 15\", fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = np.genfromtxt(f\"{CPP_DIRECTORY}/matrix.out\")\n",
    "np.savetxt(f\"{MATRIX_DIRECTORY}/sc_BioGRID_GDV_similarity.txt\", D, \n",
    "           fmt='%.7f', header=' '.join(PPI_nx), comments='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_distances = ['euclidean', 'cityblock', 'seuclidean', 'sqeuclidean', \n",
    "                 'cosine', 'correlation', 'chebyshev', 'canberra', \n",
    "                 'braycurtis', 'mahalanobis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for distance in all_distances:\n",
    "    D = cdist(GDV.values, GDV.values, distance)\n",
    "    np.savetxt(f\"{MATRIX_DIRECTORY}/sc_BioGRID_GDV_{distance}.txt\", D, \n",
    "               fmt='%.7f', header=' '.join(PPI_nx), comments='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GCV-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PPI_nx = nx.read_edgelist(f\"{PPI_DIRECTORY}/BioGRID_sc.txt\")\n",
    "GCV = graco.coefficients(PPI_nx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hellinger - single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "_SQRT2 = np.sqrt(2) \n",
    "\n",
    "def hellinger(p, q):\n",
    "    return np.sqrt(np.sum((np.sqrt(p) - np.sqrt(q)) ** 2)) / _SQRT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-0: 242.75sec\n",
      "-1-3: 245.51sec\n",
      "3-3: 240.31sec\n",
      "1-1: 236.50sec\n",
      "-1-2: 239.88sec\n",
      "-1-0: 237.58sec\n",
      "-1-1: 236.89sec\n",
      "1-2: 239.48sec\n",
      "2-1: 239.23sec\n"
     ]
    }
   ],
   "source": [
    "for order,source in set((order,source) for order,source,target in GCV.columns):\n",
    "    t1 = time.time()\n",
    "    D = cdist(np.array(GCV[order][source]), np.array(GCV[order][source]), hellinger)\n",
    "    t2 = time.time()\n",
    "    print(f'{order}-{source}: {t2-t1:.2f}sec')  \n",
    "    np.savetxt(f\"{MATRIX_DIRECTORY}/sc_BioGRID_{order}GCV{source}_hellinger.txt\", D, \n",
    "               fmt='%.7f', header=' '.join(PPI_nx), comments='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hellinger - combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_list = []\n",
    "\n",
    "for order,source in set((order,source) for order,source,target in GCV.columns):\n",
    "    df = pd.read_csv(f\"{MATRIX_DIRECTORY}/sc_BioGRID_{order}GCV{source}_hellinger.txt\", delimiter=' ')\n",
    "    D_list.append(np.array(df))\n",
    "\n",
    "D = np.nanmean(D_list, axis=0)\n",
    "np.savetxt(f\"{MATRIX_DIRECTORY}/sc_BioGRID_GCV_hellinger.txt\", D, \n",
    "               fmt='%.7f', header=' '.join(PPI_nx), comments='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TVD - individual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-0: 165.49sec\n",
      "-1-3: 161.96sec\n",
      "3-3: 162.53sec\n",
      "1-1: 162.82sec\n",
      "-1-2: 162.89sec\n",
      "-1-0: 161.32sec\n",
      "-1-1: 162.55sec\n",
      "1-2: 161.95sec\n",
      "2-1: 160.59sec\n"
     ]
    }
   ],
   "source": [
    "for order,source in set((order,source) for order,source,target in GCV.columns):\n",
    "    t1 = time.time()\n",
    "    D = cdist(np.array(GCV[order][source]), np.array(GCV[order][source]), graco.functions.tvd)\n",
    "    t2 = time.time()\n",
    "    print(f'{order}-{source}: {t2-t1:.2f}sec')  \n",
    "    np.savetxt(f\"{MATRIX_DIRECTORY}/sc_BioGRID_{order}GCV{source}_tvd.txt\", D, \n",
    "               fmt='%.7f', header=' '.join(PPI_nx), comments='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TVD - combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_list = []\n",
    "\n",
    "for order,source in set((order,source) for order,source,target in GCV.columns):\n",
    "    df = pd.read_csv(f\"{MATRIX_DIRECTORY}/sc_BioGRID_{order}GCV{source}_tvd.txt\", delimiter=' ')\n",
    "    D_list.append(np.array(df))\n",
    "\n",
    "D = np.nanmean(D_list, axis=0)\n",
    "np.savetxt(f\"{MATRIX_DIRECTORY}/sc_BioGRID_GCV_TVD.txt\", D, \n",
    "               fmt='%.7f', header=' '.join(PPI_nx), comments='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCV = GCV.fillna(0)\n",
    "all_distances = ['euclidean', 'cityblock', 'seuclidean', 'sqeuclidean', \n",
    "                 'cosine', 'correlation', 'chebyshev', 'canberra', \n",
    "                 'braycurtis', 'mahalanobis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for distance in all_distances:\n",
    "    D = cdist(GCV.values, GCV.values, distance)\n",
    "    np.savetxt(f\"{MATRIX_DIRECTORY}/sc_BioGRID_GCV_{distance}.txt\", D, \n",
    "               fmt='%.7f', header=' '.join(PPI_nx), comments='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rest - individual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_distances = ['euclidean', 'cityblock', 'sqeuclidean', \n",
    "                 'cosine', 'correlation', 'chebyshev', 'canberra', \n",
    "                 'braycurtis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1-3: 0.13sec\n",
      "0-0: 0.07sec\n",
      "1-1: 0.12sec\n",
      "3-3: 0.11sec\n",
      "-1-1: 0.12sec\n",
      "-1-0: 0.09sec\n",
      "1-2: 0.11sec\n",
      "2-1: 0.12sec\n",
      "-1-2: 0.13sec\n",
      "-1-3: 0.08sec\n",
      "0-0: 0.07sec\n",
      "1-1: 0.09sec\n",
      "3-3: 0.10sec\n",
      "-1-1: 0.10sec\n",
      "-1-0: 0.06sec\n",
      "1-2: 0.09sec\n",
      "2-1: 0.10sec\n",
      "-1-2: 0.09sec\n",
      "-1-3: 0.22sec\n",
      "0-0: 0.16sec\n",
      "1-1: 0.25sec\n",
      "3-3: 0.35sec\n",
      "-1-1: 0.23sec\n",
      "-1-0: 0.21sec\n",
      "1-2: 0.25sec\n",
      "2-1: 0.34sec\n",
      "-1-2: 0.21sec\n",
      "-1-3: 0.22sec\n",
      "0-0: 0.07sec\n",
      "1-1: 0.14sec\n",
      "3-3: 0.16sec\n",
      "-1-1: 0.18sec\n",
      "-1-0: 0.08sec\n",
      "1-2: 0.16sec\n",
      "2-1: 0.16sec\n",
      "-1-2: 0.15sec\n",
      "-1-3: 0.10sec\n",
      "0-0: 0.15sec\n",
      "1-1: 0.22sec\n",
      "3-3: 0.13sec\n",
      "-1-1: 0.19sec\n",
      "-1-0: 0.16sec\n",
      "1-2: 0.20sec\n",
      "2-1: 0.11sec\n",
      "-1-2: 0.18sec\n",
      "-1-3: 0.19sec\n",
      "0-0: 0.19sec\n",
      "1-1: 0.19sec\n",
      "3-3: 0.21sec\n",
      "-1-1: 0.11sec\n",
      "-1-0: 0.22sec\n",
      "1-2: 0.22sec\n",
      "2-1: 0.19sec\n",
      "-1-2: 0.20sec\n",
      "-1-3: 0.09sec\n",
      "0-0: 0.14sec\n",
      "1-1: 0.19sec\n",
      "3-3: 0.11sec\n",
      "-1-1: 0.22sec\n",
      "-1-0: 0.07sec\n",
      "1-2: 0.16sec\n",
      "2-1: 0.12sec\n",
      "-1-2: 0.08sec\n",
      "-1-3: 0.16sec\n",
      "0-0: 0.13sec\n",
      "1-1: 0.21sec\n",
      "3-3: 0.23sec\n",
      "-1-1: 0.30sec\n",
      "-1-0: 0.13sec\n",
      "1-2: 0.27sec\n",
      "2-1: 0.32sec\n",
      "-1-2: 0.18sec\n",
      "-1-3: 0.19sec\n",
      "0-0: 0.15sec\n",
      "1-1: 0.16sec\n",
      "3-3: 0.22sec\n",
      "-1-1: 0.22sec\n",
      "-1-0: 0.09sec\n",
      "1-2: 0.18sec\n",
      "2-1: 0.21sec\n",
      "-1-2: 0.21sec\n",
      "-1-3: 0.41sec\n",
      "0-0: 0.29sec\n",
      "1-1: 0.62sec\n",
      "3-3: 0.58sec\n",
      "-1-1: 0.61sec\n",
      "-1-0: 0.32sec\n",
      "1-2: 0.53sec\n",
      "2-1: 0.69sec\n",
      "-1-2: 0.36sec\n"
     ]
    }
   ],
   "source": [
    "for distance in all_distances:\n",
    "    for order,source in set((order,source) for order,source,target in GCV.columns):\n",
    "        D = cdist(np.array(GCV[order][source]), np.array(GCV[order][source]), distance)\n",
    "        print(f'{distance} {order: <2} {source}')  \n",
    "        np.savetxt(f\"{MATRIX_DIRECTORY}/sc_BioGRID_{order}GCV{source}_{distance}.txt\", D, \n",
    "                   fmt='%.7f', header=' '.join(PPI_nx), comments='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rest - combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "euclidean\n",
      "cityblock\n",
      "seuclidean\n",
      "sqeuclidean\n",
      "cosine\n",
      "correlation\n",
      "chebyshev\n",
      "canberra\n",
      "braycurtis\n",
      "mahalanobis\n"
     ]
    }
   ],
   "source": [
    "for distance in all_distances:\n",
    "    D_list = []\n",
    "    for order,source in set((order,source) for order,source,target in GCV.columns):\n",
    "        df = pd.read_csv(f\"{MATRIX_DIRECTORY}/sc_BioGRID_{order}GCV{source}_{distance}.txt\", delimiter=' ')\n",
    "        D_list.append(np.array(df))\n",
    "\n",
    "    D = np.nanmean(D_list, axis=0)\n",
    "    print(distance)\n",
    "    np.savetxt(f\"{MATRIX_DIRECTORY}/sc_BioGRID_gGCV_{distance}.txt\", D, \n",
    "                   fmt='%.7f', header=' '.join(PPI_nx), comments='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalized $L_p$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_lp(P,Q,p=1):\n",
    "    v1 = np.divide(P, P+Q, out=np.zeros_like(P), where=(P+Q)!=0)\n",
    "    v2 = np.divide(Q, P+Q, out=np.zeros_like(Q), where=(P+Q)!=0)\n",
    "    return np.linalg.norm(v1-v2,p)\n",
    "\n",
    "def normalized_l1(P,Q):\n",
    "    return normalized_lp(P,Q,1)\n",
    "\n",
    "def normalized_l2(P,Q):\n",
    "    return normalized_lp(P,Q,2)\n",
    "\n",
    "def normalized_linf(P,Q):\n",
    "    return normalized_lp(P,Q,np.inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalizes $L_p$ - individual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3  3\n",
      "1  2\n",
      "-1 0\n",
      "-1 1\n",
      "-1 2\n",
      "1  1\n",
      "0  0\n",
      "2  1\n",
      "-1 3\n"
     ]
    }
   ],
   "source": [
    "for order,source in set((order,source) for order,source,target in GCV.columns):\n",
    "    D = cdist(np.array(GCV[order][source]), np.array(GCV[order][source]), normalized_l1) / GCV[order][source].shape[1]\n",
    "    print(f'{order: <2} {source}')  \n",
    "    np.savetxt(f\"{MATRIX_DIRECTORY}/sc_BioGRID_{order}GCV{source}_normalized_l1.txt\", D, \n",
    "               fmt='%.7f', header=' '.join(PPI_nx), comments='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9127796\n",
      "0.8750902\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.9175424999999999\n",
      "0.8398652\n",
      "0.9237966000000001\n",
      "1.0\n",
      "0.8864963166666667\n",
      "\n"
     ]
    }
   ],
   "source": [
    "D_list = []\n",
    "for order,source in set((order,source) for order,source,target in GCV.columns):\n",
    "    df = pd.read_csv(f\"{MATRIX_DIRECTORY}/sc_BioGRID_{order}GCV{source}_normalized_l1.txt\", delimiter=' ')\n",
    "    D_list.append(np.array(df))\n",
    "    print(np.nanmax(np.array(df)))\n",
    "\n",
    "D = np.nanmean(D_list, axis=0)\n",
    "print(np.max(D))\n",
    "print()\n",
    "np.savetxt(f\"{MATRIX_DIRECTORY}/sc_BioGRID_gGCV_normalized_l1.txt\", D, \n",
    "               fmt='%.7f', header=' '.join(PPI_nx), comments='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_number_of_max_runs(GV, distance, n_clusters = 100):\n",
    "    runs = max(int(run) for run,species,db,ncluster_txt in \n",
    "             map(partial(str.split, sep='_'), os.listdir(f\"{CLUSTERS_DIRECTORY}/{GV}/{distance}\"))\n",
    "                if int(ncluster_txt.split('.')[0]) == n_clusters)\n",
    "    return runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GDV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_CLUSTERS = 2\n",
    "MAX_CLUSTERS = 100\n",
    "\n",
    "all_distances = ['mahalanobis', 'similarity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mahalanobis\n",
      "99: 507.44sec\n",
      "similarity\n",
      "99: 501.11sec\n",
      "mahalanobis\n",
      "99: 497.43sec\n",
      "similarity\n",
      "99: 499.39sec\n",
      "mahalanobis\n",
      "99: 497.79sec\n",
      "similarity\n",
      "99: 499.23sec\n",
      "mahalanobis\n",
      "99: 498.46sec\n",
      "similarity\n",
      "99: 500.07sec\n",
      "mahalanobis\n",
      "99: 498.37sec\n",
      "similarity\n",
      "99: 500.44sec\n",
      "mahalanobis\n",
      "99: 500.16sec\n",
      "similarity\n",
      "99: 500.32sec\n",
      "mahalanobis\n",
      "99: 497.62sec\n",
      "similarity\n",
      "99: 499.23sec\n",
      "mahalanobis\n",
      "99: 498.95sec\n",
      "similarity\n",
      "99: 499.71sec\n",
      "mahalanobis\n",
      "99: 497.96sec\n",
      "similarity\n",
      "99: 499.15sec\n",
      "mahalanobis\n",
      "99: 498.80sec\n",
      "similarity\n",
      "99: 500.92sec\n",
      "mahalanobis\n",
      "99: 498.55sec\n",
      "similarity\n",
      "99: 500.23sec\n",
      "mahalanobis\n",
      "99: 499.02sec\n",
      "similarity\n",
      "99: 500.84sec\n",
      "mahalanobis\n",
      "99: 498.60sec\n",
      "similarity\n",
      "99: 499.89sec\n",
      "mahalanobis\n",
      "99: 498.61sec\n",
      "similarity\n",
      "99: 500.33sec\n",
      "mahalanobis\n",
      "99: 499.67sec\n",
      "similarity\n",
      "99: 500.79sec\n",
      "mahalanobis\n",
      "99: 499.14sec\n",
      "similarity\n",
      "99: 500.67sec\n",
      "mahalanobis\n",
      "99: 499.16sec\n",
      "similarity\n",
      "99: 500.90sec\n",
      "mahalanobis\n",
      "99: 499.08sec\n",
      "similarity\n",
      "99: 501.52sec\n",
      "mahalanobis\n",
      "99: 499.56sec\n",
      "similarity\n",
      "99: 500.99sec\n",
      "mahalanobis\n",
      "99: 498.98sec\n",
      "similarity\n",
      "99: 501.10sec\n",
      "mahalanobis\n",
      "99: 498.51sec\n",
      "similarity\n",
      "99: 499.57sec\n",
      "mahalanobis\n",
      "99: 500.21sec\n",
      "similarity\n",
      "99: 500.83sec\n",
      "mahalanobis\n",
      "99: 500.78sec\n",
      "similarity\n",
      "99: 501.40sec\n",
      "mahalanobis\n",
      "99: 498.50sec\n",
      "similarity\n",
      "99: 501.44sec\n",
      "mahalanobis\n",
      "99: 500.66sec\n",
      "similarity\n",
      "99: 502.40sec\n",
      "mahalanobis\n",
      "99: 501.25sec\n",
      "similarity\n",
      "99: 502.32sec\n",
      "mahalanobis\n",
      "99: 499.02sec\n",
      "similarity\n",
      "99: 501.11sec\n",
      "mahalanobis\n",
      "99: 498.64sec\n",
      "similarity\n",
      "99: 500.32sec\n",
      "mahalanobis\n",
      "99: 499.04sec\n",
      "similarity\n",
      "99: 500.05sec\n",
      "mahalanobis\n",
      "99: 498.24sec\n",
      "similarity\n",
      "99: 500.74sec\n",
      "mahalanobis\n",
      "99: 499.34sec\n",
      "similarity\n",
      "99: 501.34sec\n",
      "mahalanobis\n",
      "99: 499.35sec\n",
      "similarity\n",
      "99: 501.54sec\n",
      "mahalanobis\n",
      "99: 498.49sec\n",
      "similarity\n",
      "99: 501.25sec\n",
      "mahalanobis\n",
      "99: 498.56sec\n",
      "similarity\n",
      "99: 500.56sec\n",
      "mahalanobis\n",
      "99: 499.06sec\n",
      "similarity\n",
      "99: 500.48sec\n",
      "mahalanobis\n",
      "99: 498.58sec\n",
      "similarity\n",
      "99: 501.06sec\n",
      "mahalanobis\n",
      "99: 499.09sec\n",
      "similarity\n",
      "99: 499.95sec\n",
      "mahalanobis\n",
      "99: 499.68sec\n",
      "similarity\n",
      "99: 500.41sec\n",
      "mahalanobis\n",
      "99: 498.53sec\n",
      "similarity\n",
      "99: 501.53sec\n",
      "mahalanobis\n",
      "99: 498.68sec\n",
      "similarity\n",
      "99: 499.59sec\n",
      "mahalanobis\n",
      "99: 498.63sec\n",
      "similarity\n",
      "99: 500.61sec\n",
      "mahalanobis\n",
      "99: 499.15sec\n",
      "similarity\n",
      "99: 500.56sec\n",
      "mahalanobis\n",
      "99: 498.85sec\n",
      "similarity\n",
      "99: 500.21sec\n",
      "mahalanobis\n",
      "99: 500.22sec\n",
      "similarity\n",
      "99: 501.81sec\n",
      "mahalanobis\n",
      "99: 497.70sec\n",
      "similarity\n",
      "99: 503.15sec\n",
      "mahalanobis\n",
      "99: 498.42sec\n",
      "similarity\n",
      "99: 500.10sec\n",
      "mahalanobis\n",
      "99: 499.58sec\n",
      "similarity\n",
      "99: 500.77sec\n",
      "mahalanobis\n",
      "99: 499.16sec\n",
      "similarity\n",
      "99: 499.82sec\n",
      "mahalanobis\n",
      "99: 499.00sec\n",
      "similarity\n",
      "99: 499.16sec\n",
      "mahalanobis\n",
      "99: 498.56sec\n",
      "similarity\n",
      "99: 500.12sec\n"
     ]
    }
   ],
   "source": [
    "# Automated\n",
    "for run in range(50):\n",
    "    for distance in all_distances:\n",
    "        print(distance)\n",
    "\n",
    "        if not os.path.exists(f\"{CLUSTERS_DIRECTORY}/GDV/{distance}\"):\n",
    "            os.makedirs(f\"{CLUSTERS_DIRECTORY}/GDV/{distance}\")\n",
    "\n",
    "        MATRIX_NAME = f\"sc_BioGRID_GDV_{distance}\"\n",
    "        with open(f\"{MATRIX_DIRECTORY}/{MATRIX_NAME}.txt\", 'r') as f:\n",
    "            line = f.readline()\n",
    "        D = np.genfromtxt(f\"{MATRIX_DIRECTORY}/{MATRIX_NAME}.txt\", skip_header=1)\n",
    "\n",
    "        int2gene = dict(enumerate(line.split()))\n",
    "\n",
    "        t1 = time.time()\n",
    "        for n_clusters in range(MIN_CLUSTERS, MAX_CLUSTERS):\n",
    "            initial_medoids = sample(range(len(D)), n_clusters)\n",
    "            kmedoids_instance = kmedoids(D, initial_medoids, data_type='distance_matrix')\n",
    "            kmedoids_instance.process()\n",
    "            \n",
    "            nr = get_number_of_max_runs('GDV', distance, n_clusters)\n",
    "\n",
    "            with open(f\"{CLUSTERS_DIRECTORY}/GDV/{distance}/{nr+1}_sc_BioGRID_{n_clusters}.txt\", 'w') as f:\n",
    "                for cluster in kmedoids_instance.get_clusters():\n",
    "                    f.write(' '.join(map(int2gene.get,cluster)) + '\\n')\n",
    "            t2 = time.time()\n",
    "            print(f'{n_clusters}: {t2-t1:.2f}sec', end='\\r')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_CLUSTERS = 2\n",
    "MAX_CLUSTERS = 100\n",
    "\n",
    "all_distances = ['seuclidean', 'canberra', 'tvd', 'hellinger']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seuclidean\n",
      "99: 498.53sec\n",
      "canberra\n",
      "99: 499.63sec\n",
      "tvd\n",
      "99: 504.42sec\n",
      "hellinger\n",
      "99: 500.98sec\n",
      "seuclidean\n",
      "99: 502.19sec\n",
      "canberra\n",
      "99: 501.07sec\n",
      "tvd\n",
      "99: 506.65sec\n",
      "hellinger\n",
      "99: 500.29sec\n",
      "seuclidean\n",
      "99: 501.09sec\n",
      "canberra\n",
      "99: 498.53sec\n",
      "tvd\n",
      "99: 503.57sec\n",
      "hellinger\n",
      "99: 499.80sec\n",
      "seuclidean\n",
      "41: 206.82sec\r"
     ]
    }
   ],
   "source": [
    "# Automated\n",
    "for run in range(50):\n",
    "    for distance in all_distances:\n",
    "        print(distance)\n",
    "\n",
    "        if not os.path.exists(f\"{CLUSTERS_DIRECTORY}/GCV/{distance}\"):\n",
    "            os.makedirs(f\"{CLUSTERS_DIRECTORY}/GCV/{distance}\")\n",
    "\n",
    "        MATRIX_NAME = f\"sc_BioGRID_GCV_{distance}\"\n",
    "        with open(f\"{MATRIX_DIRECTORY}/{MATRIX_NAME}.txt\", 'r') as f:\n",
    "            line = f.readline()\n",
    "        D = np.genfromtxt(f\"{MATRIX_DIRECTORY}/{MATRIX_NAME}.txt\", skip_header=1)\n",
    "\n",
    "        int2gene = dict(enumerate(line.split()))\n",
    "        \n",
    "        t1 = time.time()\n",
    "        for n_clusters in range(2, 100):\n",
    "            nr = get_number_of_max_runs('GCV', distance, n_clusters)\n",
    "        \n",
    "            initial_medoids = sample(range(len(D)), n_clusters)\n",
    "            kmedoids_instance = kmedoids(D, initial_medoids, data_type='distance_matrix')\n",
    "            kmedoids_instance.process()\n",
    "\n",
    "            with open(f\"{CLUSTERS_DIRECTORY}/GCV/{distance}/{nr+1}_sc_BioGRID_{n_clusters}.txt\", 'w') as f:\n",
    "                for cluster in kmedoids_instance.get_clusters():\n",
    "                    f.write(' '.join(map(int2gene.get,cluster)) + '\\n')\n",
    "            t2 = time.time()\n",
    "            print(f'{n_clusters}: {t2-t1:.2f}sec', end='\\r')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gGCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_CLUSTERS = 2\n",
    "MAX_CLUSTERS = 100\n",
    "\n",
    "all_distances = ['normalized_l1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_l1\n",
      "17: 92.52sec\r"
     ]
    }
   ],
   "source": [
    "# Automated\n",
    "for run in range(10):\n",
    "    for distance in all_distances:\n",
    "        print(distance)\n",
    "\n",
    "        if not os.path.exists(f\"{CLUSTERS_DIRECTORY}/gGCV/{distance}\"):\n",
    "            os.makedirs(f\"{CLUSTERS_DIRECTORY}/gGCV/{distance}\")\n",
    "\n",
    "        MATRIX_NAME = f\"sc_BioGRID_gGCV_{distance}\"\n",
    "        with open(f\"{MATRIX_DIRECTORY}/{MATRIX_NAME}.txt\", 'r') as f:\n",
    "            line = f.readline()\n",
    "        D = np.genfromtxt(f\"{MATRIX_DIRECTORY}/{MATRIX_NAME}.txt\", skip_header=1)\n",
    "\n",
    "        int2gene = dict(enumerate(line.split()))\n",
    "        \n",
    "        t1 = time.time()\n",
    "        for n_clusters in range(2, 100):\n",
    "            nr = get_number_of_max_runs('gGCV', distance, n_clusters) # CAREFULL !!!!!\n",
    "            initial_medoids = sample(range(len(D)), n_clusters)\n",
    "            kmedoids_instance = kmedoids(D, initial_medoids, data_type='distance_matrix')\n",
    "            kmedoids_instance.process()\n",
    "\n",
    "            with open(f\"{CLUSTERS_DIRECTORY}/gGCV/{distance}/{nr+1}_sc_BioGRID_{n_clusters}.txt\", 'w') as f:\n",
    "                for cluster in kmedoids_instance.get_clusters():\n",
    "                    f.write(' '.join(map(int2gene.get,cluster)) + '\\n')\n",
    "            t2 = time.time()\n",
    "            print(f'{n_clusters}: {t2-t1:.2f}sec', end='\\r')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graco",
   "language": "python",
   "name": "graco"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
